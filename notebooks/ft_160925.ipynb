{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0406cf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8a16cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"df = pd.read_csv('../data/processed/master_panel.csv')\n",
    "df['date'] = pd.to_datetime(df['date'])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d1bd74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional\n",
    "\n",
    "def reduce_panel_to_fraction_per_year(\n",
    "    panel: pd.DataFrame,\n",
    "    *,\n",
    "    date_col: str = \"date\",\n",
    "    id_col: str = \"permno\",\n",
    "    frac_per_year: float = 0.05,\n",
    "    random_seed: int = 42,\n",
    "    include_prev_december: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a smaller panel by sampling ~`frac_per_year` of permnos independently in each calendar year.\n",
    "    For each sampled permno in year Y, keep ALL its rows in year Y. Optionally also include rows from\n",
    "    previous December (Y-1, month=12) for these permnos so the January formation step still works.\n",
    "\n",
    "    Args:\n",
    "        panel: Full master panel with at least [id_col, date_col].\n",
    "        date_col: Name of the date column (string parseable to datetime).\n",
    "        id_col: Security identifier column (e.g., 'permno').\n",
    "        frac_per_year: Fraction of permnos to keep in each year (e.g., 0.05 for 5%).\n",
    "        random_seed: Base seed for reproducibility (varied by year under the hood).\n",
    "        include_prev_december: If True, include Y-1 December rows for the chosen permnos in year Y.\n",
    "\n",
    "    Returns:\n",
    "        A reduced DataFrame containing ~5% of the universe per year (plus prev December if requested),\n",
    "        sorted by [date_col, id_col].\n",
    "    \"\"\"\n",
    "    if not 0 < frac_per_year <= 1:\n",
    "        raise ValueError(\"frac_per_year must be in (0, 1].\")\n",
    "\n",
    "    df = panel.copy()\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "    df[\"year\"] = df[date_col].dt.year\n",
    "    df[\"month\"] = df[date_col].dt.month\n",
    "\n",
    "    reduced_frames = []\n",
    "    years = sorted(df[\"year\"].unique().tolist())\n",
    "\n",
    "    for y in years:\n",
    "        rng = np.random.RandomState(random_seed + y)\n",
    "\n",
    "        # All permnos active in this year\n",
    "        permnos_in_year = df.loc[df[\"year\"] == y, id_col].unique()\n",
    "        if len(permnos_in_year) == 0:\n",
    "            continue\n",
    "\n",
    "        # Sample ~5% permnos for this year (at least 1)\n",
    "        sample_size = max(1, int(np.ceil(len(permnos_in_year) * frac_per_year)))\n",
    "        sampled_permnos = rng.choice(permnos_in_year, size=sample_size, replace=False)\n",
    "\n",
    "        # Keep all rows for sampled permnos in this year\n",
    "        keep_y = df[(df[\"year\"] == y) & (df[id_col].isin(sampled_permnos))]\n",
    "\n",
    "        # Optionally also keep previous December for the SAME permnos, to enable Jan formation\n",
    "        if include_prev_december:\n",
    "            prev_december_mask = (df[\"year\"] == (y - 1)) & (df[\"month\"] == 12) & (df[id_col].isin(sampled_permnos))\n",
    "            keep_prev_dec = df[prev_december_mask]\n",
    "            keep_y = pd.concat([keep_prev_dec, keep_y], axis=0, ignore_index=True)\n",
    "\n",
    "        reduced_frames.append(keep_y)\n",
    "\n",
    "    reduced = pd.concat(reduced_frames, axis=0, ignore_index=True) if reduced_frames else df.iloc[0:0].copy()\n",
    "\n",
    "    # Cleanup and sort\n",
    "    reduced = reduced.drop(columns=[\"year\", \"month\"])\n",
    "    reduced = reduced.sort_values([date_col, id_col]).reset_index(drop=True)\n",
    "\n",
    "    return reduced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777677ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Reduce to ~5% per year, include previous December rows for those permnos\n",
    "small_panel = reduce_panel_to_fraction_per_year(\n",
    "    df,\n",
    "    date_col=\"date\",\n",
    "    id_col=\"permno\",\n",
    "    frac_per_year=0.05,\n",
    "    random_seed=42,\n",
    "    include_prev_december=True\n",
    ")\n",
    "\n",
    "small_panel_path = Path('../data/processed/master_panel_reduced.csv')\n",
    "small_panel.to_csv(small_panel_path, index=False)\n",
    "print(f\"Saved reduced panel to: {small_panel_path}\")\n",
    "print(small_panel.head())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54f2f144",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ftibe\\AppData\\Local\\Temp\\ipykernel_29324\\982042484.py:1: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('../data/processed/master_panel_reduced.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantiles of BidAskSpread in reduced panel:\n",
      "0.0   -0.999779\n",
      "0.1   -0.685332\n",
      "0.2   -0.409513\n",
      "0.3   -0.161538\n",
      "0.4    0.000000\n",
      "0.5    0.000000\n",
      "0.6    0.132496\n",
      "0.7    0.349844\n",
      "0.8    0.566118\n",
      "0.9    0.783006\n",
      "1.0    1.000000\n",
      "Name: BidAskSpread, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/processed/master_panel_reduced.csv')\n",
    "quantiles = np.arange(0.0, 1.1, 0.1)\n",
    "print(f\"Quantiles of BidAskSpread in reduced panel:\\n{df['BidAskSpread'].quantile(quantiles)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00a2e30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_research_kills_alpha.support.constants import PREDICTED_COL\n",
    "# drop column where PREDICTED_COL is NaN\n",
    "df = df.dropna(subset=PREDICTED_COL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd28d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-01 16:59:10,091 [INFO] RollingTrainer initialized for years 2005 to 2007 with target column: 'ret'\n"
     ]
    }
   ],
   "source": [
    "from ml_research_kills_alpha.modeling.rolling_trainer import RollingTrainer\n",
    "from ml_research_kills_alpha.modeling.algorithms.elastic_net import ElasticNetModel\n",
    "from ml_research_kills_alpha.modeling.algorithms.huber_ols import HuberRegressorModel\n",
    "from ml_research_kills_alpha.modeling.algorithms.neural_networks import FFNNModel\n",
    "\n",
    "models = [ElasticNetModel(), HuberRegressorModel(), FFNNModel(2), FFNNModel(3), FFNNModel(4), FFNNModel(5)]\n",
    "trainer = RollingTrainer(models=models, data=df, end_year=2007)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41e404bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-01 16:59:10,170 [INFO] Training models for test year 2005...\n",
      "2025-10-01 16:59:10,178 [INFO] Year 2005: Using 102 valid features out of 212 total features.\n",
      "C:\\Users\\ftibe\\OneDrive\\Desktop\\ml-research-kills-alpha\\ml_research_kills_alpha\\modeling\\rolling_trainer.py:80: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  features_data[self.year_col] = pd.to_datetime(features_data[self.year_col])\n",
      "2025-10-01 16:59:13,693 [INFO] Year 2005: Train data from start to 1998, Validation data from 1999 to 2004, Test data for 2005.\n",
      "2025-10-01 16:59:14,734 [INFO] Training model: ENET for year 2005\n",
      "2025-10-01 16:59:30,787 [INFO] Evaluating model: ENET for year 2005, month 2005-02\n",
      "2025-10-01 16:59:30,819 [INFO] Generated predictions for ENET\n",
      "2025-10-01 16:59:30,854 [INFO] Evaluating model: ENET for year 2005, month 2005-03\n",
      "2025-10-01 16:59:30,873 [INFO] Generated predictions for ENET\n",
      "2025-10-01 16:59:30,882 [INFO] Evaluating model: ENET for year 2005, month 2005-04\n",
      "2025-10-01 16:59:30,901 [INFO] Generated predictions for ENET\n",
      "2025-10-01 16:59:30,909 [INFO] Evaluating model: ENET for year 2005, month 2005-05\n",
      "2025-10-01 16:59:30,929 [INFO] Generated predictions for ENET\n",
      "2025-10-01 16:59:30,937 [INFO] Evaluating model: ENET for year 2005, month 2005-06\n",
      "2025-10-01 16:59:30,952 [INFO] Generated predictions for ENET\n",
      "2025-10-01 16:59:30,958 [INFO] Evaluating model: ENET for year 2005, month 2005-07\n",
      "2025-10-01 16:59:30,974 [INFO] Generated predictions for ENET\n",
      "2025-10-01 16:59:30,981 [INFO] Evaluating model: ENET for year 2005, month 2005-08\n",
      "2025-10-01 16:59:30,996 [INFO] Generated predictions for ENET\n",
      "2025-10-01 16:59:31,002 [INFO] Evaluating model: ENET for year 2005, month 2005-09\n",
      "2025-10-01 16:59:31,018 [INFO] Generated predictions for ENET\n",
      "2025-10-01 16:59:31,027 [INFO] Evaluating model: ENET for year 2005, month 2005-10\n",
      "2025-10-01 16:59:31,043 [INFO] Generated predictions for ENET\n",
      "2025-10-01 16:59:31,053 [INFO] Evaluating model: ENET for year 2005, month 2005-11\n",
      "2025-10-01 16:59:31,070 [INFO] Generated predictions for ENET\n",
      "2025-10-01 16:59:31,077 [INFO] Evaluating model: ENET for year 2005, month 2005-12\n",
      "2025-10-01 16:59:31,096 [INFO] Generated predictions for ENET\n",
      "2025-10-01 16:59:31,105 [INFO] Training model: OLS-H for year 2005\n",
      "2025-10-01 17:01:34,064 [INFO] Evaluating model: OLS-H for year 2005, month 2005-02\n",
      "2025-10-01 17:01:34,084 [INFO] Generated predictions for OLS-H\n",
      "2025-10-01 17:01:34,092 [INFO] Evaluating model: OLS-H for year 2005, month 2005-03\n",
      "2025-10-01 17:01:34,107 [INFO] Generated predictions for OLS-H\n",
      "2025-10-01 17:01:34,114 [INFO] Evaluating model: OLS-H for year 2005, month 2005-04\n",
      "2025-10-01 17:01:34,128 [INFO] Generated predictions for OLS-H\n",
      "2025-10-01 17:01:34,134 [INFO] Evaluating model: OLS-H for year 2005, month 2005-05\n",
      "2025-10-01 17:01:34,148 [INFO] Generated predictions for OLS-H\n",
      "2025-10-01 17:01:34,153 [INFO] Evaluating model: OLS-H for year 2005, month 2005-06\n",
      "2025-10-01 17:01:34,167 [INFO] Generated predictions for OLS-H\n",
      "2025-10-01 17:01:34,315 [INFO] Evaluating model: OLS-H for year 2005, month 2005-07\n",
      "2025-10-01 17:01:34,327 [INFO] Generated predictions for OLS-H\n",
      "2025-10-01 17:01:34,332 [INFO] Evaluating model: OLS-H for year 2005, month 2005-08\n",
      "2025-10-01 17:01:34,345 [INFO] Generated predictions for OLS-H\n",
      "2025-10-01 17:01:34,350 [INFO] Evaluating model: OLS-H for year 2005, month 2005-09\n",
      "2025-10-01 17:01:34,362 [INFO] Generated predictions for OLS-H\n",
      "2025-10-01 17:01:34,367 [INFO] Evaluating model: OLS-H for year 2005, month 2005-10\n",
      "2025-10-01 17:01:34,379 [INFO] Generated predictions for OLS-H\n",
      "2025-10-01 17:01:34,383 [INFO] Evaluating model: OLS-H for year 2005, month 2005-11\n",
      "2025-10-01 17:01:34,396 [INFO] Generated predictions for OLS-H\n",
      "2025-10-01 17:01:34,401 [INFO] Evaluating model: OLS-H for year 2005, month 2005-12\n",
      "2025-10-01 17:01:34,413 [INFO] Generated predictions for OLS-H\n",
      "2025-10-01 17:01:34,418 [INFO] Training model: FFNN2 for year 2005\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m results = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m results\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\ml-research-kills-alpha\\ml_research_kills_alpha\\modeling\\rolling_trainer.py:203\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    200\u001b[39m for year in range(self.start_year, self.end_year + 1):\n\u001b[32m    201\u001b[39m     self.logger.info(f\"Training models for test year {year}...\")\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     # Split data into training, validation, and test sets\n\u001b[32m    204\u001b[39m     train_data, val_data, test_data = self.train_val_test_split(year)\n\u001b[32m    206\u001b[39m     # Prepare data for modeling\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\ml-research-kills-alpha\\ml_research_kills_alpha\\modeling\\rolling_trainer.py:132\u001b[39m, in \u001b[36mrun_model\u001b[39m\u001b[34m(self, model, X_train, y_train, X_val, y_val, year, test_sorted, months, preds_all, train)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, model: Modeler,\n\u001b[32m    126\u001b[39m               X_train: pd.Series, y_train: pd.Series,\n\u001b[32m    127\u001b[39m               X_val: pd.Series, y_val: pd.Series,\n\u001b[32m    128\u001b[39m               year: \u001b[38;5;28mint\u001b[39m, test_sorted: pd.DataFrame,\n\u001b[32m    129\u001b[39m               months: pd.Series, preds_all: \u001b[38;5;28mlist\u001b[39m,\n\u001b[32m    130\u001b[39m               train: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m) -> \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mlist\u001b[39m]:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m \u001b[33;03m    Train and evaluate a single model for a specific year.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \u001b[39;00m\n\u001b[32m    134\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    135\u001b[39m \u001b[33;03m        model (Modeler): the model to train and evaluate.\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[33;03m        X_train (pd.Series): training features.\u001b[39;00m\n\u001b[32m    137\u001b[39m \u001b[33;03m        y_train (pd.Series): training target.\u001b[39;00m\n\u001b[32m    138\u001b[39m \u001b[33;03m        X_val (pd.Series): validation features.\u001b[39;00m\n\u001b[32m    139\u001b[39m \u001b[33;03m        y_val (pd.Series): validation target.\u001b[39;00m\n\u001b[32m    140\u001b[39m \u001b[33;03m        year (int): the test year.\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m        test_sorted (pd.DataFrame): sorted test data for evaluation.\u001b[39;00m\n\u001b[32m    142\u001b[39m \u001b[33;03m        months (pd.Series): unique months in the test data.\u001b[39;00m\n\u001b[32m    143\u001b[39m \u001b[33;03m        preds_all (list): list to append predictions to.\u001b[39;00m\n\u001b[32m    144\u001b[39m \u001b[33;03m        train (bool): whether to train the model or not - for ensembles is False.\u001b[39;00m\n\u001b[32m    145\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    146\u001b[39m     \u001b[38;5;28mself\u001b[39m.logger.info(\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m train:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\ml-research-kills-alpha\\ml_research_kills_alpha\\modeling\\algorithms\\neural_networks.py:167\u001b[39m, in \u001b[36mFFNNModel.train\u001b[39m\u001b[34m(self, X_train, y_train, X_val, y_val)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(X_train), batch_size):\n\u001b[32m    166\u001b[39m     idx = permutation[i:i+batch_size]\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     batch_X = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    168\u001b[39m     batch_y = torch.tensor(y_train[idx], dtype=torch.float32, device=device)\n\u001b[32m    169\u001b[39m     optimizer.zero_grad()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "results = trainer.run()\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
